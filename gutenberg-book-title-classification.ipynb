{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\textbf{distributional hypothesis} of linguistics states that linguistic items (sentences, paragraphs, chapters in a book, etc) with similar \\textbf{word distributions} have \\textbf{similar meanings}. As such, humans are able to learn similarities between tokens of language and infer the context in which they were spawned, sometimes extracting meaning seemingly out of thin air. This in turn is pivotal in natural language processing whenever we are concerned with constructing a statistical model of language, more specifically: classifying language tokens based on their \\textbf{word frequency content}.\n",
    "\n",
    "\n",
    "A simple example that illustrates this fact is the following: Suppose Bob and Alice are two friends. Alice and Bob are discussing what movies they have been watching recently and Alice tells Bob that she recently watched \"Die Hard\". Bob, who lives under a rock, has never heard of such a movie but he can guess from the title that she probably watched an action flick. Bob has infered this by noticing similarities between action movie titles and establishing a mental model of what an action movie title sounds like.\n",
    "\n",
    "\n",
    "![1](images/1.png)\n",
    "\n",
    "\n",
    "In general, the titles of creative works tend to contain information that encodes the escense of the \"genre\" that it belongs in, not excluding literary works, on which we have based this project. \n",
    "\n",
    "\n",
    "[Project Gutenburg](https://www.gutenberg.org/) contains a vast corpus of several book titles and category mappings that we can train machine learning models on in order to make guesses about a book category based on it's title. More specifically, our dataset contains over 200000 samples of books from several different categories as shown bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Arts & Photography               6460\n",
       "Biographies & Memoirs            4261\n",
       "Business & Money                 9965\n",
       "Calendars                        2636\n",
       "Children's Books                13605\n",
       "Christian Books & Bibles         9139\n",
       "Comics & Graphic Novels          3026\n",
       "Computers & Technology           7979\n",
       "Cookbooks, Food & Wine           8802\n",
       "Crafts, Hobbies & Home           9934\n",
       "Education & Teaching             1664\n",
       "Engineering & Transportation     2672\n",
       "Gay & Lesbian                    1339\n",
       "Health, Fitness & Dieting       11886\n",
       "History                          6807\n",
       "Humor & Entertainment            6896\n",
       "Law                              7314\n",
       "Literature & Fiction             7580\n",
       "Medical Books                   12086\n",
       "Mystery, Thriller & Suspense     1998\n",
       "Parenting & Relationships        2523\n",
       "Politics & Social Sciences       3402\n",
       "Reference                        3268\n",
       "Religion & Spirituality          7559\n",
       "Romance                          4291\n",
       "Science & Math                   9276\n",
       "Science Fiction & Fantasy        3800\n",
       "Self-Help                        2703\n",
       "Sports & Outdoors                5968\n",
       "Teen & Young Adult               7489\n",
       "Test Preparation                 2906\n",
       "Travel                          18338\n",
       "Name: booktitle, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('../data/books.csv', encoding=\"ISO-8859-1\")\n",
    "dataset.groupby('category').count()['booktitle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, our ML models cannot understand raw text data, so in order to train our models on this dataset we need to build a preprocessing pipeline for our book title samples. Here we implement the tried and true method for preprocessing text data:\n",
    "\n",
    "1. Filter all non alphabetical characters ($,%,@,#,*, etc...)\n",
    "2. Convert samples to lowercase.\n",
    "3. Disassemble the sentences to individual words by \"splitting\" on spaces.\n",
    "4. \"Stem\" samples using the porter stemmer, this consists of converting similar (inflected/derived) words such as \"love\" and \"loving\" to a common stem, \"lov\" effectively compressing our vocabulary.\n",
    "\n",
    "\n",
    "![2.jpg](images/2.jpg)\n",
    "\n",
    "\n",
    "5. Reassemble the sentence.\n",
    "\n",
    "The vastness of our dataset calls for extensive use of machine assets, therefore in order to parrallelize the above described procedure, we use python's \"concurrent\" library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207572/207572 [04:51<00:00, 711.02it/s] \n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    for i in tqdm(range(0, dataset.shape[0])):\n",
    "        book = re.sub('[^a-zA-Z]', ' ', dataset['booktitle'][i]) # Filter non alphabetical chars\n",
    "        book = book.lower() # Convert to lowercase\n",
    "        book = book.split() # Split on spaces\n",
    "        ps = PorterStemmer() # Stem\n",
    "        book = [ps.stem(word) for word in book if not word in set(stopwords.words('english'))]\n",
    "        book = ' '.join(book) \n",
    "        corpus.append(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully built our corpus we move on to sample vectorization, i.e convertion of character string data to a numerical representation our models can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization - TFIDF Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, \\underline{word frequencies} in a document contain important information about the meaning of a certain linguistic item. A simple way of encoding sentences in numerical form is using a bag-of-words model, creating dictionaries of word to word-frequency mappings for each item.\n",
    "\n",
    "\n",
    "![3.png](images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, this type of vectorization produces a highly sparse vector, meaning that most of the dictionary values are set to zero and thus the mere presence of a word in the vector object is more important than the quantity that it appears in.\n",
    "\n",
    "This mode of vectorization however contains a very significant pitfall: Several frequently occuring words such as pronouns, that don't contain any information significant to class categorization induce a skewed dataset that will affect our category predictions down the line... In order to avoid this we employ TF-IDF Normalization.\n",
    "\n",
    "TF-IDF, meaning \\underline{term frequency-inverse term frequency} is a method of normalization that multiplies term frequencies (tf term) ie how many times a word appears in a certain document with the log of the inverse document frequencies ie the number of documents that a certain word can be found in (idf term) in order to dumpen the effect of words that occur very frquently. The formula for tf idf thus, is as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "tfidf(w, d) = tf(w,d)idf(w,d) = tf(w,d)\\log \\frac{n_d}{1+df\\left(w\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "\n",
    "$tf(w, d)$: Frequency of term $w$ in document $d$\n",
    "\n",
    "$df(w)$: Document frequency of term $w$\n",
    "\n",
    "$n_d$: The total amount of documents\n",
    "\n",
    "We therefore vectorize and transform our samples according to the above by utilizing sklearn's TFidf vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is nearly ready, all that remains is to import and encode the category labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = dataset['category'].values # Get target categories\n",
    "y = label_encoder.fit_transform(y) # Encode category labels to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into a training and test set of data and labels so that we get around 2000 samples of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205496, 50618) (2076, 50618)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.01, random_state = 0)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done! We are now ready to train and test our models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we shall implement and compare several different kinds of machine learning models on the task of book category classification. Before we move forward, it is important to instantiate a dictionary of book category to class label integer mappings for lookup purposes later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               0\n",
      "Arts & Photography             0\n",
      "Biographies & Memoirs          1\n",
      "Business & Money               2\n",
      "Calendars                      3\n",
      "Children's Books               4\n",
      "Christian Books & Bibles       5\n",
      "Comics & Graphic Novels        6\n",
      "Computers & Technology         7\n",
      "Cookbooks, Food & Wine         8\n",
      "Crafts, Hobbies & Home         9\n",
      "Education & Teaching          10\n",
      "Engineering & Transportation  11\n",
      "Gay & Lesbian                 12\n",
      "Health, Fitness & Dieting     13\n",
      "History                       14\n",
      "Humor & Entertainment         15\n",
      "Law                           16\n",
      "Literature & Fiction          17\n",
      "Medical Books                 18\n",
      "Mystery, Thriller & Suspense  19\n",
      "Parenting & Relationships     20\n",
      "Politics & Social Sciences    21\n",
      "Reference                     22\n",
      "Religion & Spirituality       23\n",
      "Romance                       24\n",
      "Science & Math                25\n",
      "Science Fiction & Fantasy     26\n",
      "Self-Help                     27\n",
      "Sports & Outdoors             28\n",
      "Teen & Young Adult            29\n",
      "Test Preparation              30\n",
      "Travel                        31\n"
     ]
    }
   ],
   "source": [
    "name_map = pd.DataFrame.from_dict(dict(zip(label_encoder.classes_, \n",
    "                                           label_encoder.transform(label_encoder.classes_))), \n",
    "                                  orient='index')\n",
    "print(name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This import is useful later \n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest classifiers, as the name suggests is a model that consists of a collection of decision trees trained in parallel in order to make class predictions. This is in order to combat inaccuracies in test data that decision trees exhibit due to overfitting on the train set.\n",
    "\n",
    "More specifically, random forest classifiers use a combination of bootstrapping on the dataset and aggregation called \"bagging\" to produce a more stable random model, this consists of training each individual tree in a randomly sampled subset of the dataset and splitting each individual node on a random selection of features (bootstrapping). Similar to simple decision trees, at each step of training, we search for the optimal split based on some criterion (gini, entropy, etc)\n",
    "\n",
    "The trained classifiers are then aggregated together to make predictions based on a majority voting rule (aggregation).\n",
    "\n",
    "Classifiers that use the above described procedure are called ensemble classifiers. The generalized pipeline of such classifiers is visualized bellow:\n",
    "\n",
    "\n",
    "![4.png](images/4.png)\n",
    "\n",
    "\n",
    "We implement our random forrest model using the sklearn's random forrest api training 50 trees of maximum depth 200, using categorical cross entropy as optimal split criterion\n",
    "\n",
    "$$ -\\frac{1}{N}{\\sum^{N}_{i=1}}\\left[{y_i}log(y_i) + (1-y_i)log(1-y_i)\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:  8.1min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.8563621676334332\n",
      "Test set score: 0.5221579961464354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.25      0.31        64\n",
      "           1       0.38      0.25      0.30        32\n",
      "           2       0.66      0.58      0.62       112\n",
      "           3       0.85      0.96      0.90        24\n",
      "           4       0.24      0.75      0.36       134\n",
      "           5       0.54      0.56      0.55        77\n",
      "           6       0.64      0.58      0.61        31\n",
      "           7       0.53      0.68      0.59        60\n",
      "           8       0.77      0.79      0.78        90\n",
      "           9       0.54      0.53      0.53        83\n",
      "          10       0.67      0.38      0.48        16\n",
      "          11       0.31      0.15      0.21        26\n",
      "          12       0.75      0.25      0.38        12\n",
      "          13       0.61      0.64      0.62       123\n",
      "          14       0.53      0.44      0.48        66\n",
      "          15       0.74      0.29      0.42        90\n",
      "          16       0.63      0.59      0.61        63\n",
      "          17       0.53      0.43      0.48        90\n",
      "          18       0.65      0.72      0.68       120\n",
      "          19       0.50      0.20      0.29        20\n",
      "          20       0.54      0.26      0.35        27\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.63      0.36      0.46        33\n",
      "          23       0.67      0.32      0.43        87\n",
      "          24       0.55      0.41      0.47        39\n",
      "          25       0.52      0.53      0.53        98\n",
      "          26       0.80      0.36      0.50        33\n",
      "          27       0.17      0.04      0.06        27\n",
      "          28       0.69      0.30      0.42        66\n",
      "          29       0.67      0.36      0.47        78\n",
      "          30       0.75      0.78      0.76        27\n",
      "          31       0.55      0.78      0.64       187\n",
      "\n",
      "    accuracy                           0.52      2076\n",
      "   macro avg       0.56      0.45      0.48      2076\n",
      "weighted avg       0.56      0.52      0.51      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=200, criterion='entropy', \n",
    "                                    warm_start=True, n_jobs=4, verbose=True)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on train and test set\n",
    "print(\"Train set score:\", rf.score(X_train, y_train))\n",
    "print(\"Test set score:\", rf.score(X_test, y_test))\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test.astype(int), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model may be able to make reliable predictions on train set data but underperforms on test set data, making correct predictions only near 50% of the time failing to generalize well. Furthermore, as reported above training this model may take quite a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting classifiers are similar to ensemble classifiers in that they use a collection of weak classifiers to make predictions, except that they are trained in succession in order to improve upon the false classification the previous classifiers have made.\n",
    "\n",
    "There are many kinds of boosting classifers, but we will focus on tree based classifiers.\n",
    "\n",
    "The basis for any gradient boosting algorithm is the following: Suppose an imperfect classifier is trained (ex: a simple decision tree with low depth), the predicted class values for a single sample $x$ are \n",
    "\n",
    "\n",
    "$$F_0(x)=y_p$$\n",
    "\n",
    "This weak classifier could be any model, in our case a tree model fitted to minimize a loss function $L(y_i, F_0(x))$. A way to improve upon this classifier would be to calculate the residuals of this approximations:\n",
    "\n",
    "\n",
    "$$h_0(x) = y - F_0(x)$$\n",
    "\n",
    "where y are the ground truth labels, and fit a new model to predict these values, thus obtaining a more efficient classifier \n",
    "\n",
    "\n",
    "$$F_1(x) = F_0(x) + h_0(x)$$\n",
    "\n",
    "This can be done for an arbitrary number of steps until we are satisfied with our approximation, in general at the m-th step of approximation: \n",
    "\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + h_{m-1}(x)$$\n",
    "\n",
    "More accurately, we use a weighted sum of succesive predictors \n",
    "\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\gamma_{m-1}h_0(x)$$\n",
    "\n",
    "Where the $\\gamma_{m-1}$ coefficients are set by performing \"line search\" to find the value that minimizes the aformentioned loss function:\n",
    "\n",
    "\n",
    "$$ \\gamma_m = argmin_\\gamma\\{\\sum^n_{i=1}[L(y_i, F_{m-1}(x_i) - \\gamma h_{m-1}(x_i)]\\}$$\n",
    "\n",
    "We implement our gradient boosting classifier using the xgboost api, setting a large number of successive tree estimators of depth 50 to perform book title classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.8984408455639039\n",
      "Test set score: 0.6160886319845857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49        64\n",
      "           1       0.30      0.34      0.32        32\n",
      "           2       0.71      0.63      0.67       112\n",
      "           3       0.92      0.92      0.92        24\n",
      "           4       0.39      0.61      0.48       134\n",
      "           5       0.66      0.70      0.68        77\n",
      "           6       0.67      0.65      0.66        31\n",
      "           7       0.70      0.80      0.74        60\n",
      "           8       0.81      0.83      0.82        90\n",
      "           9       0.63      0.70      0.66        83\n",
      "          10       0.53      0.50      0.52        16\n",
      "          11       0.38      0.38      0.38        26\n",
      "          12       0.54      0.58      0.56        12\n",
      "          13       0.68      0.72      0.70       123\n",
      "          14       0.51      0.58      0.54        66\n",
      "          15       0.59      0.42      0.49        90\n",
      "          16       0.66      0.62      0.64        63\n",
      "          17       0.47      0.48      0.48        90\n",
      "          18       0.76      0.74      0.75       120\n",
      "          19       0.57      0.40      0.47        20\n",
      "          20       0.60      0.44      0.51        27\n",
      "          21       0.16      0.07      0.10        41\n",
      "          22       0.59      0.39      0.47        33\n",
      "          23       0.66      0.59      0.62        87\n",
      "          24       0.47      0.51      0.49        39\n",
      "          25       0.63      0.69      0.66        98\n",
      "          26       0.55      0.48      0.52        33\n",
      "          27       0.50      0.33      0.40        27\n",
      "          28       0.71      0.56      0.63        66\n",
      "          29       0.58      0.42      0.49        78\n",
      "          30       0.84      0.78      0.81        27\n",
      "          31       0.75      0.83      0.79       187\n",
      "\n",
      "    accuracy                           0.62      2076\n",
      "   macro avg       0.59      0.57      0.58      2076\n",
      "weighted avg       0.62      0.62      0.61      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(max_depth=50, n_estimators=100, booster='gbtree',\n",
    "                            objective='reg:logistic', gamma=0.1, rate_drop=0.3, \n",
    "                            nthread=4, verbosity=0);\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate and dump\n",
    "print(\"Train set score:\", xgb.score(X_train, y_train))\n",
    "print(\"Test set score:\", xgb.score(X_test, y_test))\n",
    "# Generate classification report\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(classification_report(y_test.astype(int), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The naive Bayes classifier model derives its name from the 18th century mathematician and theologian Thomas Bayes who formulated the Bayesian probability model that its based on.\n",
    "\n",
    "Consider a set of object classes $C_j, j=1,\\dots,n$, given a feature vector of an object $x = (x_1,x_2,\\dots,x_n)$we are interested in estimating the probability that the certain object is contained in any class given the features $x_i$ i.e $P(C_j|x_1,x_2,\\dots,x_n)$,using the bayesian probability rule we calculate the conditional probability as follows:\n",
    "\n",
    "\n",
    "$$P(C_j|x_1,x_2,\\dots,x_n) = \\frac{P(x_1,x_2,\\dots,x_n|C_j)P(C_j)}{P(x_1,x_2,\\dots,x_n)} = \\frac{P(x_1,x_2,\\dots,x_n|C_j)P(C_j)}{\\sum_i P(x_1,x_2,\\dots,x_n|C_i)P(C_i)} $$\n",
    "\n",
    "\n",
    "The \"naivety\" of naive bayes is attributed to the following assumption: we assume that any features $x_i,x_j$ are statistically independant from each other, that is\n",
    "\n",
    "\n",
    " $$ \\frac{P(x_1,x_2,\\dots,x_n|C_j)P(C_j)}{\\sum_i P(x_1,x_2,\\dots,x_n|C_i)P(C_i)} =  \n",
    "\\frac{P(x_1|C_j)P(x_2|C_j)\\dots P(x_n|C_j)P(C_j)}{\\sum_i P(x_1,x_2,\\dots,x_n|C_i)P(C_i)}  $$\n",
    "\n",
    "The above probability model is combined with a decision rule to make a final decision about the class that should be selected for $x$ to be classified in. One common rule is to pick the hypothesis that is most probable; this is known as the maximum a posteriori or MAP decision rule\n",
    "\n",
    "\n",
    "$$ argmax_{k=1,2,\\dots,n}\\{P(C_k){\\prod^n}_{i=1}P(x_i|C_k)\\} $$\n",
    "\n",
    "Training a naive bayes model is closely linked to PDF estimation. The probability densities $P(x_i|C_k)$ are not known quantities and must be approximated. Several kinds of naive bayes classifiers exist, each using a different kind of approximation technique for estimating the underlying pdf. For our model, we settled on multinomial Naive bayes which has been shown to perform well for nlp tasks. This model assumes that the underlying PDF for each class is given by the formula:\n",
    "\n",
    "$$ p(x|C_k) = \\frac{\\sum^{N}_{i=1}x_i}{\\prod^{N}_{i=1}x_i!}\\prod^{N}_{i=1}p_{ki}^{x_i} $$\n",
    "\n",
    "Where $p_{ki}$ are the trainable parameters, corresponding to probabilities of feature (word) i belonging to a document of class k.\n",
    "\n",
    "According to sklearn documentation, these parameters are estimated during training time using the formula:\n",
    "\n",
    "$$ p_{ki} = \\frac{N_{ki} + \\alpha}{N_k + n\\alpha} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$N_{ki}$: Is the total count of occurances of feature i for class k\n",
    "\n",
    "$N_{k}$: Is the total count of all features for class k$\n",
    "\n",
    "$\\alpha\\in[0,1]$: Is Lidstone's smoothing parameter used to avoid overfitting\n",
    "\n",
    "This makes for a relatively efficient yet simple model with a very fast training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.7290896173161522\n",
      "Test set score: 0.6257225433526011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.39      0.45        64\n",
      "           1       0.46      0.38      0.41        32\n",
      "           2       0.65      0.75      0.69       112\n",
      "           3       0.83      0.83      0.83        24\n",
      "           4       0.49      0.72      0.59       134\n",
      "           5       0.52      0.62      0.56        77\n",
      "           6       0.88      0.68      0.76        31\n",
      "           7       0.72      0.77      0.74        60\n",
      "           8       0.79      0.87      0.83        90\n",
      "           9       0.58      0.70      0.63        83\n",
      "          10       0.25      0.06      0.10        16\n",
      "          11       0.43      0.23      0.30        26\n",
      "          12       0.71      0.42      0.53        12\n",
      "          13       0.61      0.77      0.68       123\n",
      "          14       0.60      0.71      0.65        66\n",
      "          15       0.60      0.41      0.49        90\n",
      "          16       0.65      0.67      0.66        63\n",
      "          17       0.53      0.51      0.52        90\n",
      "          18       0.69      0.79      0.74       120\n",
      "          19       0.62      0.25      0.36        20\n",
      "          20       0.77      0.37      0.50        27\n",
      "          21       0.44      0.17      0.25        41\n",
      "          22       0.69      0.33      0.45        33\n",
      "          23       0.61      0.59      0.60        87\n",
      "          24       0.61      0.64      0.62        39\n",
      "          25       0.65      0.63      0.64        98\n",
      "          26       0.61      0.52      0.56        33\n",
      "          27       0.42      0.19      0.26        27\n",
      "          28       0.77      0.45      0.57        66\n",
      "          29       0.46      0.31      0.37        78\n",
      "          30       0.81      0.81      0.81        27\n",
      "          31       0.73      0.89      0.80       187\n",
      "\n",
      "    accuracy                           0.63      2076\n",
      "   macro avg       0.62      0.54      0.56      2076\n",
      "weighted avg       0.62      0.63      0.61      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(alpha=0.1, fit_prior=True, class_prior=None)\n",
    "nb.fit(X_train, y_train)\n",
    "    \n",
    "# Evaluate and dump\n",
    "print(\"Train set score:\", nb.score(X_train, y_train))\n",
    "print(\"Test set score:\", nb.score(X_test, y_test))\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = nb.predict(X_test)\n",
    "print(classification_report(y_test.astype(int), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression model simulates how brain neurons work to make decisions. Essentialy stimuli from different nerve endings are aggregated in a neuron that fires only if the total intensity of the stimulation is great enough to surpass a certain threshold.\n",
    "\n",
    "![5.png](images/5.png)\n",
    "\n",
    "Similarly to the process described previously, an input vector $x = [x_1,x_2,\\dots,x_n]$ (stimulus) is applied to a weight vector  $w$ to produce a stimulus output: \n",
    "$$z = w_0x_0 + w_1x_1 + ... + w_nx_n$$\n",
    "\n",
    "This output is passed through an _activation function_ $\\phi(z)$ that transforms the real number output to represent a probability between $[0,1]$. This function is usually either the sigmoid function:\n",
    "\n",
    "$$\\phi(z) = \\frac{1}{1 + e^-z} $$\n",
    "\n",
    "Another often used function in logistic regression is the reLU function and its derivative, leaky reLU, boasting simplicity that results in smaller evaluation time.\n",
    "\n",
    "The final output from the activation function is compared to a threshold value and a dicision about the objects class is made\n",
    "\n",
    "$$y = g({z}) = \n",
    " \\begin{cases}\n",
    "  1 & \\text{if $\\phi(z) \\ge 0.5$}\\\\\n",
    "  0 & \\text{otherwise.}\n",
    "   \\end{cases}\n",
    "$$\n",
    "The subsequent model is optimized by maximizing the subsequent likelihood function\n",
    "\n",
    "$$L(\\mathbf{w}) = P(\\mathbf{y} \\mid \\mathbf{x};\\mathbf{w}) = \\prod_{i=1}^{n} P\\big(y^{(i)} \\mid x^{(i)}; \\mathbf{w}\\big) = \\prod^{n}_{i=1}\\bigg(\\phi\\big(z^{(i)}\\big)\\bigg)^{y^{(i)}} \\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg)^{1-y^{(i)}},$$\n",
    "\n",
    "Which is the same as maximizing log-likelihood:\n",
    "\n",
    "$$l(\\mathbf{w}) = \\log L(\\mathbf{w}) = \\sum^{n}_{i=1} y^{(i)} \\log \\bigg(\\phi\\big(z^{(i)}\\big)\\bigg) + \\big( 1 - y^{(i)}\\big) \\log \\big(1-\\phi\\big(z^{(i)}\\big)\\big)$$\n",
    "\n",
    "For better optimization performance, this loss function is ammended with an **l2 normalization** term\n",
    "\n",
    "$$J(\\mathbf{w}) =  \\sum_{i=1}^{m} \\Bigg[ - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg) \\Bigg] + \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$$\n",
    "\n",
    "\n",
    "The most famous methods to solve the above unconstrained optimization problem are **gradient-based** methods, such as batch gradient descent which involves iteratively improving the model by iterativelly setting the weights:\n",
    "\n",
    "$$w_j := w_j + \\eta \\sum^{n}_{i=1} \\big( y^{(i)} - \\phi\\big(z^{(i)}\\big)\\big)x_j^{(i)}$$\n",
    "\n",
    "Factoring in the normalization term: \n",
    "\n",
    "$$w_j := w_j + \\eta \\sum^{n}_{i=1} \\big( y^{(i)} - \\phi\\big(z^{(i)}\\big)\\big)x_j^{(i)} - \\eta \\lambda w_j$$\n",
    "\n",
    "Where $\\eta$ is a parameter called \"learning rate\",the value of which corresponds to the \"step size\" when progressing towards a final estimation. This procedure iteratively proceeds to a classifer vector that correctly categorizes each class sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 22 epochs took 15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   14.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.7105929069178962\n",
      "Train set score: 0.634393063583815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.42      0.47        64\n",
      "           1       0.41      0.38      0.39        32\n",
      "           2       0.73      0.72      0.73       112\n",
      "           3       0.91      0.83      0.87        24\n",
      "           4       0.46      0.72      0.56       134\n",
      "           5       0.59      0.64      0.61        77\n",
      "           6       0.76      0.61      0.68        31\n",
      "           7       0.70      0.82      0.75        60\n",
      "           8       0.78      0.81      0.79        90\n",
      "           9       0.56      0.65      0.60        83\n",
      "          10       0.54      0.44      0.48        16\n",
      "          11       0.56      0.38      0.45        26\n",
      "          12       0.55      0.50      0.52        12\n",
      "          13       0.67      0.73      0.70       123\n",
      "          14       0.63      0.64      0.63        66\n",
      "          15       0.68      0.44      0.54        90\n",
      "          16       0.67      0.59      0.63        63\n",
      "          17       0.51      0.48      0.49        90\n",
      "          18       0.76      0.79      0.78       120\n",
      "          19       0.50      0.35      0.41        20\n",
      "          20       0.60      0.56      0.58        27\n",
      "          21       0.57      0.20      0.29        41\n",
      "          22       0.67      0.55      0.60        33\n",
      "          23       0.71      0.62      0.66        87\n",
      "          24       0.51      0.59      0.55        39\n",
      "          25       0.62      0.64      0.63        98\n",
      "          26       0.75      0.55      0.63        33\n",
      "          27       0.39      0.26      0.31        27\n",
      "          28       0.71      0.53      0.61        66\n",
      "          29       0.45      0.38      0.42        78\n",
      "          30       0.83      0.74      0.78        27\n",
      "          31       0.73      0.90      0.81       187\n",
      "\n",
      "    accuracy                           0.63      2076\n",
      "   macro avg       0.63      0.58      0.59      2076\n",
      "weighted avg       0.64      0.63      0.63      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='sag',max_iter=200,random_state=450,\n",
    "                                             n_jobs=4, verbose=True)\n",
    "lr.fit(X_train, y_train)\n",
    "        \n",
    "# Evaluate on train and test set\n",
    "print(\"Train set score:\", lr.score(X_train, y_train))\n",
    "print(\"Train set score:\", lr.score(X_test, y_test))\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test.astype(int), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier performs steadilly well across all categories and has a relatively brief training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What constitutes differentiates a deep model from any other machine learning model? The short answer is: automatic feature extraction, whereas in \"classic\" machine learning features of an abstract object are constructed by the programmer \"by hand\" in deep learning the features themselves are trainable parameters. Many deep learning models are inspired by the brain's function, more specifically convolutional neural networks are inspired by the visual cortex, while lstm's simualate a simple model of short term memory.\n",
    "\n",
    "**Data preprocessing:**\n",
    "\n",
    "In order to construct deep neural models, we use the keras api. Keras provides its own api for efficiently preprocessing language segments in order to convert them to sequence vectors and subsequently vectorized. More importantly the keras api provides a trainable embedding layer that translates the processed data to \\textbf{word embedding vectors}. Word embeddings are a vector space model of data representation frequently used in information retrieval, where natural language words are translated to numerical vectors as depicted bellow: \n",
    "\n",
    "![6.png](images/6.png)\n",
    "\n",
    "The embedding layer of any nlp model is trained so that the embedding vectors of similar or related words are clustered near each other, retaining a sense of geometric closeness in accordance to the distributional hypothesis discussed above. This is crucial to the high performance of the models as we will note moving forwards.\n",
    "\n",
    "Lets move on to the preprocessing pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "## Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "X = tokenizer.texts_to_sequences(corpus)\n",
    "max_features=len(tokenizer.word_index) + 1\n",
    "\n",
    "## Pad sequences\n",
    "X = sequence.pad_sequences(X, maxlen=100)\n",
    "\n",
    "## Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = to_categorical(label_encoder.fit_transform(dataset['category'].values))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.01, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, LSTM, Conv1D, GlobalMaxPooling1D,\\\n",
    "                         Dense, Dropout, Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to logistic regression described before, artificial neural networks simulate biological neurons firing in tandem to encode information and make predictions. Each individual neuron is described by a vector of weights $w_{j}$ where j is the neuron index in the layer. The neuron outputs, consist of the dot product of the input vector filtered through an activation function, usually the logistic sigmoid function we discussed previously:\n",
    "\n",
    "\\begin{equation}\n",
    "a_j = \\sigma(z_j) = \\sigma(\\sum_{i=1}^{D}w_{ji}x_i + w_{j0})\n",
    "\\end{equation}\n",
    "\n",
    "Where $z_j$ is our \"activation\" dot product plus a bias term $z_j = w_{j}x + w_{j0}$, or if we use extended vector notation, i.e $x' = [1, x_1, x_2, \\dots]$ then $z_j = [w_j, w_{j0}]x'$. We can express the mapping of input feature vector into layer activations as a matrix multiplication with the weight matrix $W$ formed by stacking the neuron weight vectors as columns:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{a} = \\sigma(\\mathbf{z}=W \\mathbf{x})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In feed-forward neural networks layers of neurons are stacked in a sequence in order for the output of the previous layer to be fed as input to the next one. These networks map any input vector (stimulus) to an output vector (response) by feeding them through a number of \"hidden layers\" as described before. In general for the activations at the k-th layer we derive the following formula:\n",
    "\n",
    "$$ a^L = \\sigma(z^L) = \\sigma(^La^{L-1})$$\n",
    "\n",
    "![7.png](images/7.png)\n",
    "\n",
    "Each layers weights are fitted to a train set of data vectors in order to learn to make classifications by a process called \\textbf{backpropagation}. As any other machine learning model, in order to learn to item classification a loss function $l(x)$ needs to be minimized. Assume for now . In order to do this we use a gradient based optimizer to update the weights iteratively. To employ this, we need to first calculate the derivative of the loss function with respect to each layers weights. Suppose that we have an L-layered model with sigmoid activation functions, the derivative of the loss function with respect to the weights of the last layer (output layer) is calculated according to the chain rule as:\n",
    "\n",
    "$$ \\frac{\\partial l(x)}{\\partial W^L}= \\frac{\\partial a^L}{\\partial W^L}\\frac{\\partial l(x)}{\\partial a^L} = \\frac{\\partial z^L}{\\partial W^L}\\frac{\\partial a^L}{\\partial z^L}\\frac{\\partial l(x)}{\\partial a^L} $$\n",
    "\n",
    "\n",
    "If we assume for now that $ l(x) = \\frac{1}{2N}\\sum^N_{i=1}(a^L - y)^T(a^L - y)$, meaning our loss function is set to M.S.E. then:\n",
    "\n",
    "$$ \\frac{\\partial l(x)}{\\partial a^L} = (a^L - y)$$\n",
    "\n",
    "$$ \\frac{\\partial a^L}{\\partial z^L} = \\sigma'(z^L) = \\sigma(z^L)(1 - \\sigma(z^L))$$\n",
    "\n",
    "$$ \\frac{\\partial z^L}{\\partial W^L} = a^{L-1}$$\n",
    "\n",
    "And finaly, putting it all together:\n",
    "\n",
    "$$ \\frac{\\partial l(x)}{\\partial W^L} = a^{L-1}\\sigma'(z^L)(a^L - y)$$\n",
    "\n",
    "In order to propagate this result backwards we merely have to establish that for the k-th layer:\n",
    "\n",
    "$$ \\frac{\\partial l(x)}{\\partial a^k} = \\frac{\\partial z^{k+1}}{\\partial a^{k+1}}\\frac{\\partial a^{k+1}}{\\partial z^{k+1}}\\frac{\\partial l(x)}{\\partial a^{k+1}} = W^k\\sigma'(z^{k+1})\\frac{\\partial l(x)}{\\partial a^{k+1}} $$\n",
    "\n",
    "And:\n",
    "\n",
    "$$ \\frac{\\partial l(x)}{\\partial W^k} = \\frac{\\partial z^k}{\\partial W^k}\\frac{\\partial a^k}{\\partial z^k}\\frac{\\partial l(x)}{\\partial a^k}  = a^k\\sigma'(z^k)\\frac{\\partial l(x)}{\\partial a^k}$$\n",
    "\n",
    "This means that we can progress backwards calculating the gradients for each function by applying a matrix multiplication to the previous layer we calculated. The weights are updated at each step according to the formula:\n",
    "\n",
    "$$ W^k := W^k + \\eta \\frac{\\partial l(x)}{\\partial W^k} $$\n",
    "\n",
    "Where $\\eta$ is the learning rate parameter. By this process we set up a hidden state in our network that allows it to extract hidden features from abstract items of informations and efficiently classify them to the given categories.\n",
    "\n",
    "We build a model with a single hidden layer that follow the embedding layer and then followed by an appropriate size output layer. \"Dropout\" refers to the tendency of neurons to \"die out\" after a certain amount of iterations with a specified probability. We use this to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "205496/205496 [==============================] - 54s 262us/step - loss: 0.0935 - accuracy: 0.9742\n",
      "Epoch 2/2\n",
      "205496/205496 [==============================] - 51s 247us/step - loss: 0.0612 - accuracy: 0.9809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd3442b5be0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Sequential()\n",
    "\n",
    "## EMBEDDING LAYER ##\n",
    "ann.add(Embedding(max_features, 50, input_length=100))\n",
    "ann.add(Flatten())\n",
    "\n",
    "## HIDDEN LAYER ##    \n",
    "ann.add(Dense(256))\n",
    "ann.add(Dropout(0.1))\n",
    "ann.add(Activation('relu'))\n",
    "    \n",
    "## OUTPUT LAYER ##\n",
    "ann.add(Dense(32))\n",
    "ann.add(Activation('softmax'))\n",
    "\n",
    "ann.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "ann.fit(X_train, y_train, batch_size=256, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.7224568848055437\n",
      "Test set score: 0.6189788053949904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.40      0.39        60\n",
      "           1       0.34      0.32      0.33        34\n",
      "           2       0.72      0.67      0.70       121\n",
      "           3       0.92      0.92      0.92        24\n",
      "           4       0.66      0.59      0.62       150\n",
      "           5       0.64      0.48      0.55       102\n",
      "           6       0.65      0.74      0.69        27\n",
      "           7       0.82      0.70      0.75        70\n",
      "           8       0.86      0.76      0.81       101\n",
      "           9       0.66      0.59      0.62        93\n",
      "          10       0.38      0.35      0.36        17\n",
      "          11       0.38      0.53      0.44        19\n",
      "          12       0.42      0.45      0.43        11\n",
      "          13       0.67      0.72      0.69       116\n",
      "          14       0.70      0.57      0.63        81\n",
      "          15       0.47      0.58      0.52        72\n",
      "          16       0.62      0.71      0.66        55\n",
      "          17       0.49      0.48      0.49        91\n",
      "          18       0.79      0.75      0.77       127\n",
      "          19       0.25      0.71      0.37         7\n",
      "          20       0.44      0.50      0.47        24\n",
      "          21       0.17      0.39      0.24        18\n",
      "          22       0.42      0.70      0.53        20\n",
      "          23       0.61      0.77      0.68        69\n",
      "          24       0.69      0.36      0.48        74\n",
      "          25       0.55      0.57      0.56        94\n",
      "          26       0.39      0.57      0.46        23\n",
      "          27       0.37      0.37      0.37        27\n",
      "          28       0.50      0.73      0.59        45\n",
      "          29       0.38      0.47      0.42        64\n",
      "          30       0.70      0.76      0.73        25\n",
      "          31       0.87      0.75      0.81       215\n",
      "\n",
      "    accuracy                           0.62      2076\n",
      "   macro avg       0.56      0.59      0.57      2076\n",
      "weighted avg       0.64      0.62      0.62      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = ann.predict_classes(X_test)\n",
    "y_true_test = np.argmax(y_test, axis=1)\n",
    "y_pred_train = ann.predict_classes(X_train)\n",
    "y_true_train = np.argmax(y_train,axis=1)\n",
    "\n",
    "print(\"Train set score:\", accuracy_score(y_pred_train, y_true_train))\n",
    "print(\"Test set score:\", accuracy_score(y_pred_test, y_true_test))\n",
    "print(classification_report(y_pred_test, y_true_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are similar to ANN's in that they propagate stimuli from the input layer towards the output layer filtering input through a series of matrices with set weights and biases but instead of using matrix multiplications the weight matrices, \"kernels\" are \\textbf{convoluted} with the given input. We introduce the convolution operation symbolized with the $*$ operator as:\n",
    "\n",
    "$$ (x*y)(n) = \\sum^N_{k=1}x[k]y[n - k]$$\n",
    "\n",
    "Where x,y are two discrete time signals, i.e sample vectors.\n",
    "\n",
    "This operation produces a new set of feature samples where each coefficient is produced by the weighted sum of the kernel function with a \"neighborhood\" of local input samples, capturing a sense of information locality in our input features. This kernel of weights \"slides\" across the input vector producing a new \"feature map\"\n",
    "\n",
    "The input to a convolutional layer is usually taken to be three-dimensional: the height, weight and\n",
    "number of channels. In the first layer this input is convolved with a set of M1 three-dimensional filters\n",
    "applied over all the input channels (in other words, the third dimension of the filter map is always equal to\n",
    "the number of channels in the input) to create the feature output map. Consider now a one-dimensional\n",
    "input vector $x$ of size $N$. This is mapped to an output feature map by convolving with each individual filter, thus the i-th element of the h-th feature is computed as:\n",
    "\n",
    "$$ a^1(i,h) = (x*w_h)(i) =\\sum^N_{k=1}x[k]w^1_h[i-k]$$\n",
    "\n",
    "The feature map is then passed through an activation function $h(.)$ and the final output for the convolutional layer is: $f^1(i,h) = h(a^1(i,h))$\n",
    "\n",
    "This is then fed to the rest of the convolutional layers accordingly:\n",
    "\n",
    "$$ a^l(i, h) = (w^l_h ∗ f^{l−1})(i) = \\sum^{\\infty}_{j=-\\infty}\\sum^{M_l}_{k=1}w^l_h[j,k]f^{l-1}(i-j,k)$$\n",
    "\n",
    "And similarily each layer is passed through an activation function $f^l(i,h) = h(a^l(i,h))$\n",
    "\n",
    "In between convolutional layers there may be some \\underline{pooling layers} that essentially downsample the data by \"pooling\" a neighborhood of features into a single feature usually by mean, or max operation (\"max pooling\") that is by representing this neighborhood of features using the largest one in value (max).\n",
    "\n",
    "![8.png](images/8.png)\n",
    "\n",
    "Finally, after a series of convolutional layers the output is flattened and fed into a dense layer of neurons, where the output prediction is made. This whole process is visualized bellow\n",
    "\n",
    "![9.png](images/9.png)\n",
    "\n",
    "The convolutional layer weights are again set here with a process similar to backpropagation we discussed previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "205496/205496 [==============================] - 174s 846us/step - loss: 0.0970 - accuracy: 0.9739\n",
      "Epoch 2/2\n",
      "205496/205496 [==============================] - 182s 888us/step - loss: 0.0665 - accuracy: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd32f4c0b38>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "## EMBEDDING LAYER ##\n",
    "cnn.add(Embedding(max_features, 50, input_length=100))\n",
    "cnn.add(Dropout(0.2))\n",
    " \n",
    "## CONVOLUTIONAL LAYER ##\n",
    "cnn.add(Conv1D(250, 10, padding='valid', activation='relu',\n",
    "                 strides=1));\n",
    "cnn.add(GlobalMaxPooling1D())\n",
    " \n",
    "## DENSE LAYER ##\n",
    "cnn.add(Dense(64))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Activation('relu'))\n",
    "    \n",
    "## OUTPUT LAYER ##\n",
    "cnn.add(Dense(32))\n",
    "cnn.add(Activation('softmax'))\n",
    "\n",
    "cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn.fit(X_train, y_train, batch_size=256, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.694621793124927\n",
      "Test set score: 0.6122350674373795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.53      0.44        45\n",
      "           1       0.38      0.43      0.40        28\n",
      "           2       0.76      0.69      0.72       123\n",
      "           3       0.96      0.88      0.92        26\n",
      "           4       0.67      0.53      0.59       169\n",
      "           5       0.62      0.49      0.55        97\n",
      "           6       0.68      0.64      0.66        33\n",
      "           7       0.75      0.60      0.67        75\n",
      "           8       0.82      0.81      0.82        91\n",
      "           9       0.66      0.62      0.64        89\n",
      "          10       0.25      0.50      0.33         8\n",
      "          11       0.38      0.50      0.43        20\n",
      "          12       0.50      0.46      0.48        13\n",
      "          13       0.73      0.60      0.66       149\n",
      "          14       0.61      0.50      0.55        80\n",
      "          15       0.36      0.70      0.47        46\n",
      "          16       0.60      0.67      0.63        57\n",
      "          17       0.49      0.46      0.47        96\n",
      "          18       0.75      0.74      0.74       122\n",
      "          19       0.15      0.38      0.21         8\n",
      "          20       0.52      0.54      0.53        26\n",
      "          21       0.15      0.32      0.20        19\n",
      "          22       0.45      0.65      0.54        23\n",
      "          23       0.59      0.69      0.63        74\n",
      "          24       0.56      0.34      0.42        65\n",
      "          25       0.62      0.54      0.58       114\n",
      "          26       0.48      0.62      0.54        26\n",
      "          27       0.11      0.21      0.15        14\n",
      "          28       0.58      0.79      0.67        48\n",
      "          29       0.29      0.43      0.35        53\n",
      "          30       0.74      0.87      0.80        23\n",
      "          31       0.90      0.78      0.83       216\n",
      "\n",
      "    accuracy                           0.61      2076\n",
      "   macro avg       0.55      0.58      0.55      2076\n",
      "weighted avg       0.65      0.61      0.62      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = cnn.predict_classes(X_test)\n",
    "y_true_test = np.argmax(y_test, axis=1)\n",
    "y_pred_train = cnn.predict_classes(X_train)\n",
    "y_true_train = np.argmax(y_train,axis=1)\n",
    "\n",
    "print(\"Train set score:\", accuracy_score(y_pred_train, y_true_train))\n",
    "print(\"Test set score:\", accuracy_score(y_pred_test, y_true_test))\n",
    "print(classification_report(y_pred_test, y_true_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short term memory networks are a subcategory of recurrent neural networks, a type of neural network that feeds the output it produces in a given instance of a sequence back to the input layers effectively learning associations in sequential data. An lstm cell, is responsible for withholding a certain \"memory\" about a series of samples it has been presented with by the following process:\n",
    "\n",
    "The lstm cell maintains a certain \"cell state\" at the instance $t$ : $C_t$. Suppose at this instant that the output of the cell is $o_t$ and the input of the sequence is the vector $x_t$. The cell also contains a hidden state that is at the given instance computed as such:\n",
    "\n",
    "$$ h_t = tanh(C_t)o_t $$\n",
    "\n",
    "The output of the cell at the same instance is calculated based on the current input and previous state as follows:\n",
    "\n",
    "$$ o_t = \\sigma(x_tU^o + h_{t-1}W^o)$$\n",
    "\n",
    "Where $U^o$ and $W^o$ are trainable parameters. It follows from this that the cell state is essentially a scaling parameter that signifies how crucial the current outputs are to the task of predicting the next sample value. For this reason the cell state is to be updated at each step of the way. From the previous hidden state and the current inputs a \"suggested\" cell state emerges\n",
    "\n",
    "$$ \\tilde{C_t} = tanh(x_tU^g + h_{t-1}W^g) $$\n",
    "\n",
    "Where $U^g$ and $W^g$ are trainable \"gate\" parameters. The current state and the suggested state are then scaled by the parameters $f_t$ and $i_t$ via a Hadamard product $\\circ$ (element wise multiplication) and finally the new cell state is:\n",
    "\n",
    "$$ \\tilde{C_t} = tanh(f_t \\circ C_t + i_t \\circ \\tilde{C_t}) $$\n",
    "\n",
    "In this essence $f_t$ is a \"forgetting\" factor and $i_t$ is a current sample \"retention\" factor. These factors are in themselves dependant on the current input and hidden state and are to be learned from the input sequence, thus they are computed as such:\n",
    "\n",
    "$$ f_t = \\sigma(x_tU^f + h_{t-1}W^f) $$\n",
    "\n",
    "$$ i_t = \\sigma(x_tU^i + h_{t-1}W^i) $$\n",
    "\n",
    "Where $U^f,W^f,U^i,W^i$ are trainable parameters.\n",
    "\n",
    "All of the previous trainable parameters are set via  the same process we described in previous deep models.\n",
    "\n",
    "The stracture of an LSTM cell is displayed bellow:\n",
    "\n",
    "![11.png](images/11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "205496/205496 [==============================] - 284s 1ms/step - loss: 0.0960 - accuracy: 0.9738\n",
      "Epoch 2/2\n",
      "205496/205496 [==============================] - 361s 2ms/step - loss: 0.0662 - accuracy: 0.9796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd3216e42e8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "\n",
    "## EMBEDDING LAYER ##\n",
    "lstm.add(Embedding(max_features, 50, input_length=100))\n",
    "lstm.add(Dropout(0.2))\n",
    "    \n",
    "## LSTM LAYER #\n",
    "lstm.add(LSTM(124, dropout=0.2, recurrent_dropout=0.2))\n",
    "    \n",
    "## OUTPUT LAYER ##\n",
    "lstm.add(Dense(32))\n",
    "lstm.add(Activation('softmax'))\n",
    "\n",
    "lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm.fit(X_train, y_train, batch_size=256, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.694621793124927\n",
      "Test set score: 0.6122350674373795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.53      0.44        45\n",
      "           1       0.38      0.43      0.40        28\n",
      "           2       0.76      0.69      0.72       123\n",
      "           3       0.96      0.88      0.92        26\n",
      "           4       0.67      0.53      0.59       169\n",
      "           5       0.62      0.49      0.55        97\n",
      "           6       0.68      0.64      0.66        33\n",
      "           7       0.75      0.60      0.67        75\n",
      "           8       0.82      0.81      0.82        91\n",
      "           9       0.66      0.62      0.64        89\n",
      "          10       0.25      0.50      0.33         8\n",
      "          11       0.38      0.50      0.43        20\n",
      "          12       0.50      0.46      0.48        13\n",
      "          13       0.73      0.60      0.66       149\n",
      "          14       0.61      0.50      0.55        80\n",
      "          15       0.36      0.70      0.47        46\n",
      "          16       0.60      0.67      0.63        57\n",
      "          17       0.49      0.46      0.47        96\n",
      "          18       0.75      0.74      0.74       122\n",
      "          19       0.15      0.38      0.21         8\n",
      "          20       0.52      0.54      0.53        26\n",
      "          21       0.15      0.32      0.20        19\n",
      "          22       0.45      0.65      0.54        23\n",
      "          23       0.59      0.69      0.63        74\n",
      "          24       0.56      0.34      0.42        65\n",
      "          25       0.62      0.54      0.58       114\n",
      "          26       0.48      0.62      0.54        26\n",
      "          27       0.11      0.21      0.15        14\n",
      "          28       0.58      0.79      0.67        48\n",
      "          29       0.29      0.43      0.35        53\n",
      "          30       0.74      0.87      0.80        23\n",
      "          31       0.90      0.78      0.83       216\n",
      "\n",
      "    accuracy                           0.61      2076\n",
      "   macro avg       0.55      0.58      0.55      2076\n",
      "weighted avg       0.65      0.61      0.62      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = cnn.predict_classes(X_test)\n",
    "y_true_test = np.argmax(y_test, axis=1)\n",
    "y_pred_train = cnn.predict_classes(X_train)\n",
    "y_true_train = np.argmax(y_train,axis=1)\n",
    "\n",
    "print(\"Train set score:\", accuracy_score(y_pred_train, y_true_train))\n",
    "print(\"Test set score:\", accuracy_score(y_pred_test, y_true_test))\n",
    "print(classification_report(y_pred_test, y_true_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - Using our models to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model performance is not very impressive at first glance... This could be due to the large amount of possible book categories that are often quite indistinguishable from each other and ambiguous title data. This problem has been tackled before, with [this paper](http://cs229.stanford.edu/proj2015/127_report.pdf) lowering the amount of book category classifications and also using visual data from book covers to make the classification, however the reported accuracy was only marginally improved.\n",
    "\n",
    "However, in practical scenarios even our most underperforming models tend to rank the most likely categories in a way that makes sense (the top 5 categories usually contain the correct classification). Models like these could be useful in an automatic recomendation system.\n",
    "\n",
    "It is important to note also that the paper we previously cited reports that the tested human accuracy for this problem was only around 73%.\n",
    "\n",
    "In order however to use the trained models we must invert some of the transforms we applied when preprocessing our data, to extract the correct categories in text format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with raw text data input from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = \"Data Mining Concepts & techniques\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocess the text to the proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = book.lower()\n",
    "book = book.split()\n",
    "ps = PorterStemmer()\n",
    "book = [ps.stem(word) for word in book if not word in set(stopwords.words('english'))]\n",
    "book = ' '.join(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Transform the input to vector form using the tfidf vectorizer we created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.transform([book])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(x).reshape((32,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create an ordered dictionary of the top ten most likely categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to probability mappings\n",
    "from collections import OrderedDict\n",
    "prob_classes = {k: v for k, v in zip(label_encoder.classes_, probs)}\n",
    "prob_classes = OrderedDict(reversed(sorted(prob_classes.items(), key=lambda x: x[1])[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7fd319683a90>,\n",
       "  <matplotlib.axis.XTick at 0x7fd3196833c8>,\n",
       "  <matplotlib.axis.XTick at 0x7fd3196832b0>,\n",
       "  <matplotlib.axis.XTick at 0x7fd31965a7b8>,\n",
       "  <matplotlib.axis.XTick at 0x7fd31965ac88>,\n",
       "  <matplotlib.axis.XTick at 0x7fd3195ec1d0>,\n",
       "  <matplotlib.axis.XTick at 0x7fd3195ec748>,\n",
       "  <matplotlib.axis.XTick at 0x7fd31965a860>,\n",
       "  <matplotlib.axis.XTick at 0x7fd3195eccf8>,\n",
       "  <matplotlib.axis.XTick at 0x7fd3195f22b0>],\n",
       " <a list of 10 Text xticklabel objects>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot class probabilities bar graph\n",
    "plt.bar(range(len(prob_classes)), list(prob_classes.values()), align='center')\n",
    "plt.xticks(range(len(prob_classes)), list(prob_classes.keys()), rotation=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction demo using deep models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this process again using one of our deep learning models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PorterStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-04927bdf6659>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbook\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PorterStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "# Read book title\n",
    "book = \"Principles of statistical learning\"\n",
    "\n",
    "# Filter book title data \n",
    "book = book.lower()\n",
    "book = book.split()\n",
    "ps = PorterStemmer()\n",
    "book = [ps.stem(word) for word in book if not word in set(stopwords.words('english'))]\n",
    "book = ' '.join(book)\n",
    "\n",
    "# Tokenize and prepare sequence\n",
    "x = sequence.pad_sequences(tokenizer.texts_to_sequences([book]), maxlen=100)\n",
    "probs = lstm.predict_proba(x).reshape((32,)) \n",
    "   \n",
    "# Create class to probability mappings\n",
    "prob_classes = {k: v for k, v in zip(label_encoder.classes_, probs)}\n",
    "prob_classes = OrderedDict(reversed(sorted(prob_classes.items(), key=lambda x: x[1])[-10:]))\n",
    "\n",
    "# Plot class probabilities bar graph\n",
    "plt.bar(range(len(prob_classes)), list(prob_classes.values()), align='center')\n",
    "plt.xticks(range(len(prob_classes)), list(prob_classes.keys()), rotation=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
